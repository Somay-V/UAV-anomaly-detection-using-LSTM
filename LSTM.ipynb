{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('UAV_DATA.csv')  # Replace with your file name\n",
    "\n",
    "# Ensure timestamps are sorted\n",
    "df = df.sort_values('timestamp')\n",
    "\n",
    "# Normalize the features\n",
    "scaler = MinMaxScaler()\n",
    "features = df.drop(['label', 'timestamp'], axis=1)\n",
    "df['label'] = df['label'].map({'benign':0,'jamming':1, 'spoofing':2})  # Adjust columns as needed\n",
    "labels = df['label']  # Replace with the column indicating labels\n",
    "\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Create sequences\n",
    "def create_sequences(data, labels, seq_length=10):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i+seq_length])\n",
    "        y.append(labels[i+seq_length - 1])  # Label for the last element in the sequence\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "sequence_length = 10\n",
    "X, y = create_sequences(features_scaled, labels, sequence_length)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution\n",
    "print(df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Callable, Tuple\n",
    "\n",
    "def GAOA(Materials_no: int, Max_iter: int, fobj: Callable, dim: int, lb: float, ub: float, C3: float, C4: float, max_fes: int) -> Tuple[np.ndarray, float, np.ndarray]:\n",
    "    # Initialization\n",
    "    Max_iter = round(max_fes / Materials_no)\n",
    "    FES = 0\n",
    "    C1, C2 = 2, 6\n",
    "    index = 0\n",
    "    u, l = 0.9, 0.1  # Parameters in Eq. (12)\n",
    "    \n",
    "    # Initial positions and other variables\n",
    "    X = lb + np.random.rand(Materials_no, dim) * (ub - lb)  # Initial positions Eq. (4)\n",
    "    den = np.random.rand(Materials_no, dim)  # Eq. (5)\n",
    "    vol = np.random.rand(Materials_no, dim)\n",
    "    acc = lb + np.random.rand(Materials_no, dim) * (ub - lb)  # Eq. (6)\n",
    "    \n",
    "    # Evaluate initial solutions\n",
    "    Y = np.array([fobj(X[i, :]) for i in range(Materials_no)])\n",
    "    FES += Materials_no\n",
    "\n",
    "    # Sort and select the best\n",
    "    sorted_indices = np.argsort(Y)\n",
    "    Scorebest = Y[sorted_indices[0]]\n",
    "    Xbest = X[sorted_indices[0], :]\n",
    "    den_best = den[sorted_indices[0], :]\n",
    "    vol_best = vol[sorted_indices[0], :]\n",
    "    acc_best = acc[sorted_indices[0], :]\n",
    "    acc_norm = acc.copy()\n",
    "    Convergence_curve = np.zeros(Max_iter)\n",
    "    Convergence_curve[index] = Scorebest\n",
    "    index += 1\n",
    "    num_agents = int(0.1 * Materials_no)\n",
    "\n",
    "    # Random walk parameters\n",
    "    max_step_initial = 0.1 * (ub - lb)\n",
    "    decay_rate = 2\n",
    "    neighborhood_size = 5\n",
    "    t = 1\n",
    "\n",
    "    while t <= Max_iter and FES < max_fes:\n",
    "        # Update parameters\n",
    "        TF = np.exp((t - Max_iter) / Max_iter)  # Eq. (8)\n",
    "        TF = min(TF, 1)\n",
    "\n",
    "        max_step = max_step_initial * np.exp(-decay_rate * TF)\n",
    "\n",
    "        d = np.exp((Max_iter - t) / Max_iter) - (t / Max_iter)  # Eq. (9)\n",
    "        acc = acc_norm.copy()\n",
    "        \n",
    "        # Calculate G, which combines Xbest and the mean of X across all agents\n",
    "        G = 0.25 * (Xbest + np.tile(np.mean(X[:Materials_no, :], axis=0), (Materials_no, 1)))\n",
    "\n",
    "        # Neighborhood selection and updates for den, vol, acc\n",
    "        for i in range(Materials_no):\n",
    "            neighbors = np.random.choice(np.delete(np.arange(Materials_no), i), neighborhood_size - 1, replace=False)\n",
    "            neighbor_avg = np.mean(np.vstack([X[neighbors, :], G[i, :]]), axis=0)\n",
    "\n",
    "            den[i, :] += C1 * np.random.rand() * acc_norm[i, :] * (neighbor_avg - X[i, :]) * d\n",
    "            vol[i, :] += C1 * np.random.rand() * acc_norm[i, :] * (G[i, :] - X[i, :]) * d\n",
    "\n",
    "            if TF < 0.5:  # Collision phase\n",
    "                mr = np.random.randint(Materials_no)\n",
    "                acc_temp = (den[mr, :] + vol[mr, :] * acc[mr, :]) / (np.random.rand() * den[i, :] * vol[i, :])\n",
    "            else:  # Non-collision phase\n",
    "                acc_temp = (den_best + vol_best * acc_best) / (np.random.rand() * den[i, :] * vol[i, :])\n",
    "\n",
    "            # Normalize acceleration\n",
    "            acc_norm[i, :] = ((u * (acc_temp - np.min(acc_temp))) / (np.max(acc_temp) - np.min(acc_temp))) + l\n",
    "\n",
    "        # Random walk step\n",
    "        if TF > 0.5:\n",
    "            for i in range(num_agents):\n",
    "                step_size = max_step * np.random.rand(dim)\n",
    "                direction = np.random.choice([-1, 1], dim)\n",
    "                X[i, :] += direction * step_size\n",
    "\n",
    "        # Boundary check for X\n",
    "        X = fun_checkpositions(dim, X, Materials_no, lb, ub)\n",
    "\n",
    "        # Generate new positions Xnew\n",
    "        Xnew = np.zeros_like(X)\n",
    "        for i in range(Materials_no):\n",
    "            if TF < 0.5:\n",
    "                mrand = np.random.randint(Materials_no)\n",
    "                Xnew[i, :] = X[i, :] + C1 * np.random.rand() * acc_norm[i, :] * (X[mrand, :] - X[i, :]) * d\n",
    "            else:\n",
    "                p = 2 * np.random.rand() - C4\n",
    "                T = C3 * TF\n",
    "                T = min(T, 1)\n",
    "                Xnew[i, :] = Xbest + C2 * np.random.rand() * acc_norm[i, :] * (p * Xbest - X[i, :]) * d\n",
    "\n",
    "        # Apply boundary check to Xnew\n",
    "        Xnew = fun_checkpositions(dim, Xnew, Materials_no, lb, ub)\n",
    "\n",
    "        # Evaluate new solutions and update X, Y\n",
    "        for i in range(Materials_no):\n",
    "            v = fobj(Xnew[i, :])\n",
    "            FES += 1\n",
    "            if v < Y[i]:\n",
    "                X[i, :] = Xnew[i, :]\n",
    "                Y[i] = v\n",
    "\n",
    "        # Update best score and position if found\n",
    "        var_Ybest = np.min(Y)\n",
    "        var_index = np.argmin(Y)\n",
    "        Convergence_curve[index] = var_Ybest\n",
    "        index += 1\n",
    "        if var_Ybest < Scorebest:\n",
    "            Scorebest = var_Ybest\n",
    "            Xbest = X[var_index, :]\n",
    "            den_best = den[var_index, :]\n",
    "            vol_best = vol[var_index, :]\n",
    "            acc_best = acc_norm[var_index, :]\n",
    "\n",
    "        t += 1\n",
    "\n",
    "    return Xbest, Scorebest, Convergence_curve\n",
    "\n",
    "def fun_checkpositions(dim: int, vec_pos: np.ndarray, var_no_group: int, lb: float, ub: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Ensure that each position in vec_pos is within the bounds [lb, ub].\n",
    "    \"\"\"\n",
    "    vec_pos = np.clip(vec_pos, lb, ub)\n",
    "    return vec_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "def objective(params):\n",
    "    \"\"\"\n",
    "    Objective function for GAOA. Trains an LSTM model and returns the negative validation accuracy.\n",
    "    Args:\n",
    "        params: List of hyperparameters to optimize [num_units, dropout_rate, learning_rate].\n",
    "    Returns:\n",
    "        Negative validation accuracy (as GAOA minimizes the objective).\n",
    "    \"\"\"\n",
    "    num_units1 = int(params[0])        # Number of LSTM units\n",
    "    num_units2 = int(params[1])        # Number of LSTM units\n",
    "    dropout_rate = np.clip(params[2], 0.0, 1.0)        # Dropout rate\n",
    "    learning_rate = params[3]        # Learning rate\n",
    "\n",
    "    # Define LSTM model\n",
    "    model = Sequential([\n",
    "        LSTM(num_units1, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "        Dropout(dropout_rate),\n",
    "        LSTM(num_units2),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(3, activation='softmax')  # Assuming 3 classes\n",
    "    ])\n",
    "    \n",
    "    # Compile model\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model (early stopping to prevent overfitting)\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0, validation_data=(X_val, y_val),callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate model\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = y_pred.argmax(axis=1)\n",
    "    accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "    \n",
    "    return -accuracy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 4  # Number of hyperparameters: [num_units1, num_units2, dropout_rate, learning_rate]\n",
    "lb = np.array([32, 32, 0.1, 1e-4])    # Lower bounds\n",
    "ub = np.array([64, 64, 0.5, 1e-2])    # Upper bounds\n",
    "\n",
    "Materials_no = 10  # Number of candidate solutions\n",
    "Max_iter = 20      # Maximum iterations\n",
    "C3 = 2             # GAOA-specific parameter\n",
    "C4 = 0.5           # GAOA-specific parameter\n",
    "max_fes = 200      # Maximum function evaluations\n",
    "\n",
    "# Run GAOA\n",
    "best_params, best_score, convergence_curve = GAOA(\n",
    "    Materials_no=Materials_no,\n",
    "    Max_iter=Max_iter,\n",
    "    fobj=objective,\n",
    "    dim=dim,\n",
    "    lb=lb,\n",
    "    ub=ub,\n",
    "    C3=C3,\n",
    "    C4=C4,\n",
    "    max_fes=max_fes\n",
    ")\n",
    "\n",
    "# Output results\n",
    "print(\"Best Hyperparameters found by GAOA:\", best_params)\n",
    "print(\"Best Validation Accuracy (GAOA):\", -best_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_units1 = int(best_params[0])\n",
    "num_units2 = int(best_params[1])\n",
    "dropout_rate = best_params[2]\n",
    "learning_rate = best_params[3]\n",
    "# Best Hyperparameters found by GAOA: [5.09384143e+01 3.54073999e+01 3.15575453e-01 8.01389167e-03]\n",
    "# Best Validation Accuracy (GAOA): 0.9983432736911863\n",
    "# Define the optimized LSTM model\n",
    "\n",
    "final_model = Sequential([\n",
    "        LSTM(num_units1, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "        Dropout(dropout_rate),\n",
    "        LSTM(num_units2),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(3, activation='softmax')  # Assuming 3 classes\n",
    "    ])\n",
    "\n",
    "# Compile the optimized model\n",
    "final_model.compile(optimizer=Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the optimized model\n",
    "history = final_model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = final_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred_probs = final_model.predict(X_test)\n",
    "\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[\"Benign\", \"Jamming\", \"Spoofing\"],\n",
    "            yticklabels=[\"Benign\", \"Jamming\", \"Spoofing\"])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Benign\", \"Jamming\", \"Spoofing\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve, auc, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "class_names = {0: 'Benign', 1: 'Jamming', 2: 'Spoofing'}\n",
    "\n",
    "n_classes = len(class_names)\n",
    "y_test_binarized = label_binarize(y_test, classes=np.arange(n_classes))\n",
    "\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "roc_auc = {}\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], y_pred_probs[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    plt.plot(fpr[i], tpr[i], label=f\"{class_names[i]} (AUC = {roc_auc[i]:.3f})\")\n",
    "\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_binarized.ravel(), y_pred_probs.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"], linestyle='--', label=f\"Micro-average (AUC = {roc_auc['micro']:.3f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label=\"Chance\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred) \n",
    "fpr_class = {}\n",
    "fnr_class = {}\n",
    "detection_rate = {}\n",
    "\n",
    "for i in range(n_classes):\n",
    "    TP = conf_matrix[i, i]\n",
    "    FP = conf_matrix[:, i].sum() - TP\n",
    "    FN = conf_matrix[i, :].sum() - TP\n",
    "    TN = conf_matrix.sum() - (TP + FP + FN)\n",
    "\n",
    "    fpr_class[class_names[i]] = FP / (FP + TN) if (FP + TN) > 0 else 0\n",
    "    fnr_class[class_names[i]] = FN / (FN + TP) if (FN + TP) > 0 else 0\n",
    "    detection_rate[class_names[i]] = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "\n",
    "for class_name in class_names.values():\n",
    "    print(f\"{class_name}: FPR = {fpr_class[class_name]:.2f}, FNR = {fnr_class[class_name]:.2f}, Detection Rate = {detection_rate[class_name]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
